# -*- coding: utf-8 -*-
"""DAwRPython_BoKho_CustomerSegmentation_Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAZJQwNVaVkgjEYSCzukQOJv9CuDQVXD
"""

!gdown --id 14HD0hqUU3AsQZkNr14VlsyR52RxJPEEJ

!pip install kneed

"""# IMPORTING LIBRARIES AND DATASET"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.impute import KNNImputer

# Read data
creditcard_df = pd.read_csv('/content/Marketing_data.csv')

"""# EXPLORATORY DATA ANALYSIS"""

creditcard_df.head(10)

creditcard_df.columns

creditcard_df.shape

creditcard_df.info()

unique_value_dict = dict()
for col in creditcard_df.columns:
    unique_value_dict[col] = creditcard_df[col].value_counts().count()

pd.Series(unique_value_dict).to_frame().rename(columns={0: 'Numebr of unique values'})

# Drop column CUST_ID
creditcard_df.drop(columns=['CUST_ID'], axis=1, inplace=True)

creditcard_df.describe().T

creditcard_df.corr().T

sns.clustermap(creditcard_df.corr(), cmap = "YlGnBu", dendrogram_ratio = (0.3, 0.3), annot = True, linewidths = .8, figsize = (22,18))
plt.show()

plt.figure(figsize = (15, 12))

plot2 = sns.scatterplot(x = "BALANCE", y = "CREDIT_LIMIT", data = creditcard_df);
plot2.set(xlabel = "Balance (Log(x+1))", ylabel = "Credit Limit (Log(x+1))");

plt.figure(figsize=(20,60))
for i in range(len(creditcard_df.columns)-1):
  plt.subplot(16, 2, i+1)
  sns.distplot(creditcard_df[creditcard_df.columns[i]], kde_kws={"color": "r", "lw": 1, "label": "KDE"}, hist_kws={"color": "b"})
  plt.title(creditcard_df.columns[i])

plt.tight_layout()

"""# PREPROCESSING

##Handling Null Values
"""

sns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap="rainbow")
plt.show()

creditcard_df.isnull().sum()

# Replace value null -> value mean
creditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()
creditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].mean()

# Heatmap graph showing the number of nulls --> No more null value
sns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")
plt.show()

"""##Check duplicate

"""

# Check duplicate data
import ipywidgets
from ipywidgets import interact
def duplicate_values(df):
    print("Duplicate check...", sep='')
    duplicate_values = df.duplicated(subset=None, keep='first').sum()
    if duplicate_values > 0:
        df.drop_duplicates(keep='first', inplace=True)
        print(duplicate_values, " Duplicates were dropped!",'\n',
             '*'*100, 'red', sep='')
    else:
        print("There are no duplicates",'\n',
              '*'*100, 'red', sep='')    
duplicate_values(creditcard_df)

"""##Handling Outliers"""

def outlier_percent(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    minimum = Q1 - (1.5 * IQR)
    maximum = Q3 + (1.5 * IQR)
    num_outliers =  np.sum((data < minimum) |(data > maximum))
    num_total = data.count()
    return (num_outliers/num_total)*100

non_categorical_data = creditcard_df.copy()
for column in non_categorical_data.columns:
    data = non_categorical_data[column]
    percent = str(round(outlier_percent(data), 2))
    print(f'Outliers in "{column}": {percent}%')

for column in non_categorical_data.columns:
    data = non_categorical_data[column]
    
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    minimum = Q1 - (1.5 * IQR)
    maximum = Q3 + (1.5 * IQR)
 
    outliers = ((data < minimum) |(data > maximum))
    non_categorical_data[column].loc[outliers] = np.nan
    
non_categorical_data.isnull().sum()

# Imputation
imputer = KNNImputer()
imp_data = pd.DataFrame(imputer.fit_transform(non_categorical_data), columns=non_categorical_data.columns)
imp_data.isnull().sum()

imp_data.duplicated().sum()

# Standardization
creditcard_df_scaled = pd.DataFrame(StandardScaler().fit_transform(imp_data), columns=imp_data.columns)
creditcard_df_scaled.describe()

"""##Handling Duplicate Values"""

creditcard_df_scaled.duplicated().sum()

df_dup = creditcard_df_scaled[creditcard_df_scaled.duplicated(keep=False)]

df_dup = df_dup.groupby(list(df_dup)).apply(lambda x: tuple(x.index)).tolist()
df_dup

creditcard_df = creditcard_df.drop([creditcard_df.index[643],creditcard_df.index[1778],creditcard_df.index[883]])

creditcard_df_scaled.drop_duplicates(keep='first', inplace=True, ignore_index=False)

creditcard_df_scaled

"""# CLUSTERING WITH KMEANS

"""

kmean_set = {'init':'random', 'n_init':10, 'max_iter':300, 'random_state':1}

List_Kmeans = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmean_set )
    kmeans.fit(creditcard_df_scaled)
    List_Kmeans.append(kmeans.inertia_)

# Elbow
from kneed import KneeLocator, DataGenerator
kl = KneeLocator(range(1, 11), List_Kmeans, curve = 'convex', direction = 'decreasing')
kl.elbow

plt.style.use('fivethirtyeight')
plt.plot(range(1, 11), List_Kmeans, marker = '^', c='b',  ms = 9, mfc = 'r')
plt.xticks(range(1, 11))
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

plt.axvline(x=kl.elbow, color ='gray', label = 'axvline - full height', ls = '--')

plt.show()

# Silhouette Coefficients
from sklearn.metrics import silhouette_score
silhouette_coefficients_Kmeans = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, **kmean_set )
    kmeans.fit(creditcard_df_scaled)
    score_Kmeans = silhouette_score(creditcard_df_scaled, kmeans.labels_)
    silhouette_coefficients_Kmeans.append(score_Kmeans)

plt.style.use('fivethirtyeight')
plt.plot(range(2, 11), silhouette_coefficients_Kmeans, marker = '^', c = 'b', ms = 9, mfc = 'r')
plt.xticks(range(2, 11))
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.show()

# Calinski_Harabasz Coefficient
from sklearn import metrics
from sklearn.metrics import calinski_harabasz_score
calinski_harabaz_Kmeans = []
for k in range (2,11):
    kmeans = KMeans(n_clusters=k,init= "random", random_state = 1)
    kmeans.fit(creditcard_df_scaled)
    
    metrics.calinski_harabasz_score(creditcard_df_scaled, kmeans.labels_)
    calinski_harabaz_Kmeans.append(metrics.calinski_harabasz_score(creditcard_df_scaled, kmeans.labels_))

plt.style.use("fivethirtyeight")
plt.plot(range(2,11), calinski_harabaz_Kmeans, marker = '^', c = 'b', ms = 9, mfc = 'r')
plt.xticks(range(2,11))
plt.xlabel("Number of Clusters")
plt.ylabel("calinski_harabasz")
plt.show()

creditcard_df_scaled

kmeans = KMeans(3)
kmeans.fit(creditcard_df_scaled)
labels_Kmeans = kmeans.labels_

creditcard_df.columns

cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [creditcard_df.columns])
cluster_centers

labels_Kmeans.shape # Labels associated to each data point

labels_Kmeans.max()

labels_Kmeans.min()

y_kmeans = kmeans.fit_predict(creditcard_df_scaled)
y_kmeans

creditcard_df.shape

# Obtain the principal components 
pca = PCA(n_components=2)
principal_comp_Kmeans = pca.fit_transform(creditcard_df_scaled)
principal_comp_Kmeans

# Create a dataframe with the two components
pca_df_Kmeans = pd.DataFrame(data = principal_comp_Kmeans, columns =['pca1','pca2'])
pca_df_Kmeans.head()

# Concatenate the clusters labels to the dataframe
pca_df_Kmeans = pd.concat([pca_df_Kmeans,pd.DataFrame({'cluster':labels_Kmeans})], axis = 1)
pca_df_Kmeans.head()

pca_df_Kmeans = pca_df_Kmeans.replace(0,3)
pca_df_Kmeans = pca_df_Kmeans.replace(1,4)
pca_df_Kmeans = pca_df_Kmeans.replace(2,5)

pca_df_Kmeans.head()

pca_df_Kmeans = pca_df_Kmeans.replace(3,2)
pca_df_Kmeans = pca_df_Kmeans.replace(4,0)
pca_df_Kmeans = pca_df_Kmeans.replace(5,1)

plt.figure(figsize=(10,10))
ax = sns.scatterplot(x="pca1", y="pca2", hue = "cluster", data = pca_df_Kmeans, palette =['red','green','blue'])
plt.show()

df_cluster_with_kmean = creditcard_df.copy()

df_cluster_with_kmean.head(5)

labels_Kmeans.shape

df_cluster_with_kmean['cluster'] = labels_Kmeans

df_cluster_with_kmean.tail(5)

df_cluster_with_kmean.info()

ax = sns.countplot(x=df_cluster_with_kmean.cluster)
plt.show()

df_cluster_with_kmean.shape

df_cluster_with_kmean.to_csv('df_cluster_with_kmean.csv')

"""# APPLY ANN AND CLUSTERING WITH KMEANS

"""

from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from keras.optimizers import SGD

encoding_dim = 7

input_df = Input(shape=(17,))

# Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal distribution 
x = Dense(encoding_dim, activation='relu')(input_df)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)
x = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(x)

encoded = Dense(10, activation='relu', kernel_initializer = 'glorot_uniform')(x)

x = Dense(2000, activation='relu', kernel_initializer = 'glorot_uniform')(encoded)
x = Dense(500, activation='relu', kernel_initializer = 'glorot_uniform')(x)

decoded = Dense(17, kernel_initializer = 'glorot_uniform')(x)

# autoencoder
autoencoder = Model(input_df, decoded)

#encoder - used for our dimention reduction
encoder = Model(input_df, encoded)

autoencoder.compile(optimizer= 'adam', loss='mean_squared_error')

creditcard_df_scaled.shape

autoencoder.fit(creditcard_df_scaled, creditcard_df_scaled, batch_size = 128, epochs = 25,  verbose = 1)

autoencoder.summary()

autoencoder.save_weights('autoencoder.h5')

pred = encoder.predict(creditcard_df_scaled)

pred.shape

pred

kmean_set = {'init':'random', 'n_init':10, 'max_iter':300, 'random_state':1}

List_KmeanANN = []
for k in range(1, 11):   
    kmeans = KMeans(n_clusters=k, **kmean_set ) 
    kmeans.fit(pred)
    List_KmeanANN.append(kmeans.inertia_)

# Inertia
from kneed import KneeLocator, DataGenerator
kl = KneeLocator(range(1, 11), List_KmeanANN, curve = 'convex', direction = 'decreasing')
kl.elbow

plt.style.use('fivethirtyeight')
plt.plot(range(1, 11), List_KmeanANN, marker = '^', c='b',  ms = 9, mfc = 'r')
plt.xticks(range(1, 11))
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.axvline(x=kl.elbow, color ='gray', label = 'axvline - full height', ls = '--')

plt.show()

# Silhouette Coefficient
from sklearn.metrics import silhouette_score
silhouette_coefficient_KmeanANN = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, **kmean_set )
    kmeans.fit(pred)
    score2 = silhouette_score(pred, kmeans.labels_)
    silhouette_coefficient_KmeanANN.append(score2)

plt.style.use('fivethirtyeight')
plt.plot(range(2, 11), silhouette_coefficient_KmeanANN, marker = '^', c = 'b', ms = 9, mfc = 'r')
plt.xticks(range(2, 11))
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Coefficient')
plt.show()

# Calinski_Harabasz Coefficient
from sklearn import metrics
from sklearn.metrics import calinski_harabasz_score
calinski_harabaz_KmeanANN = []
for k in range (2,11):
    kmeans = KMeans(n_clusters=k,init= "random", random_state = 1)
    kmeans.fit(pred)
    
    metrics.calinski_harabasz_score(pred, kmeans.labels_)
    calinski_harabaz_KmeanANN.append(metrics.calinski_harabasz_score(pred, kmeans.labels_))

plt.style.use("fivethirtyeight")
plt.plot(range(2,11), calinski_harabaz_KmeanANN, marker = '^', c = 'b', ms = 9, mfc = 'r')
plt.xticks(range(2,11))
plt.xlabel("Number of Clusters")
plt.ylabel("calinski_harabasz")
plt.show()

"""Choose K = 3"""

kmeans = KMeans(3)
kmeans.fit(pred)
labels = kmeans.labels_
y_kmeans = kmeans.fit_predict(pred)

pred

df_cluster_KmeanANN = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis = 1)
df_cluster_KmeanANN.head()

creditcard_df.isnull().sum()

df_cluster_KmeanANN.info()

pca = PCA(n_components=2)
prin_comp = pca.fit_transform(pred)
pca_df_KmeanANN = pd.DataFrame(data = prin_comp, columns =['pca1','pca2'])
pca_df_KmeanANN.head()

pca_df_KmeanANN = pd.concat([pca_df_KmeanANN,pd.DataFrame({'cluster':labels})], axis = 1)
pca_df_KmeanANN.head()

plt.figure(figsize=(10,10))
ax = sns.scatterplot(x="pca1", y="pca2", hue = "cluster", data = pca_df_KmeanANN, palette =['red','green','blue'])
plt.show()

creditcard_df.info()

df_cluster_with_kmean_ANN= creditcard_df.copy()
df_cluster_with_kmean_ANN['cluster'] = labels

df_cluster_with_kmean_ANN.info()

df_cluster_with_kmean_ANN.cluster.value_counts()

ax = sns.countplot(x=df_cluster_with_kmean_ANN.cluster)
plt.show()

df_cluster_with_kmean_ANN.to_csv('df_cluster_with_kmean_ANN.csv')